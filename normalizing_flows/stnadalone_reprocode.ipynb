{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opacus patch is done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mrathodmansi\u001B[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: wandb version 0.10.20 is available!  To upgrade, please run:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.18<br/>\n                Syncing run <strong style=\"color:#cdcd00\">hearty-deluge-20</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/rathodmansi/privacy_police\" target=\"_blank\">https://wandb.ai/rathodmansi/privacy_police</a><br/>\n                Run page: <a href=\"https://wandb.ai/rathodmansi/privacy_police/runs/1m6dpbcw\" target=\"_blank\">https://wandb.ai/rathodmansi/privacy_police/runs/1m6dpbcw</a><br/>\n                Run data is saved locally in <code>/Users/mansirathod/Documents/Github_Repository/Differential-Privacy/normalizing_flows/wandb/run-20210225_194502-1m6dpbcw</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from opacus import PrivacyEngine\n",
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# import flows as fnn\n",
    "# from dataset_loader import get_datasets, get_input_size\n",
    "import patch_opacus\n",
    "\n",
    "wandb.init(project='privacy_police')\n",
    "config = wandb.config\n",
    "\n",
    "print('Loading the dataset')\n",
    "\n",
    "\n",
    "def get_mnist_datasets(random_seed, alpha=1e-6):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        transforms.Lambda(lambda x: alpha + (1 - 2 * alpha) * x),\n",
    "        transforms.Lambda(lambda x: torch.log(x / (1.0 - x))),\n",
    "        transforms.Lambda(lambda x: torch.flatten(x))\n",
    "        ])\n",
    "    train_val = MNIST('../data', train=True, download=True, transform=transform)\n",
    "    test = MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train, val = random_split(train_val, [50000, 10000], generator=torch.Generator().manual_seed(random_seed))\n",
    "\n",
    "    # Returns 50K / 10K / 10K sized datasets\n",
    "    return train, val, test\n",
    "\n",
    "def get_datasets(dataset_name, random_seed):\n",
    "    \"\"\"\n",
    "    Returns train, val, test dataset\n",
    "    \"\"\"\n",
    "    dataset_name = dataset_name.lower()\n",
    "    mapping = {\n",
    "       'mnist': get_mnist_datasets\n",
    "    #    'adult' : get_adult_datasets,\n",
    "    #    'pums' : get_pums_datasets,\n",
    "    #    'power' : get_power_datasets\n",
    "    # }\n",
    "    if dataset_name not in mapping:\n",
    "        err_msg = f\"Unknown dataset '{dataset_name}'. Please choose one in {list(mapping.keys())}.\"\n",
    "        raise ValueError(err_msg)\n",
    "\n",
    "    return mapping[dataset_name](random_seed)\n",
    "\n",
    "def get_input_size(dataset_name):\n",
    "    \"\"\"\n",
    "    Returns the size of input\n",
    "    \"\"\"\n",
    "    dataset_name = dataset_name.lower()\n",
    "    mapping = {\n",
    "        'mnist': 28 * 28,\n",
    "        'adult': 6,\n",
    "        'pums': 4,\n",
    "        'power': 8\n",
    "    }\n",
    "    if dataset_name not in mapping:\n",
    "        err_msg = f\"Unknown dataset '{dataset_name}'. Please choose one in {list(mapping.keys())}.\"\n",
    "        raise ValueError(err_msg)\n",
    "\n",
    "    return mapping[dataset_name]\n",
    "\n",
    "print('MADE & MAF Architecture')\n",
    "\n",
    "def get_mask(in_features, out_features, in_flow_features, mask_type=None):\n",
    "    \"\"\"\n",
    "    mask_type: input | None | output\n",
    "\n",
    "    See Figure 1 for a better illustration:\n",
    "    https://arxiv.org/pdf/1502.03509.pdf\n",
    "    \"\"\"\n",
    "    if mask_type == 'input':\n",
    "        in_degrees = torch.arange(in_features) % in_flow_features\n",
    "    else:\n",
    "        in_degrees = torch.arange(in_features) % (in_flow_features - 1)\n",
    "\n",
    "    if mask_type == 'output':\n",
    "        out_degrees = torch.arange(out_features) % in_flow_features - 1\n",
    "    else:\n",
    "        out_degrees = torch.arange(out_features) % (in_flow_features - 1)\n",
    "\n",
    "    return (out_degrees.unsqueeze(-1) >= in_degrees.unsqueeze(0)).float()\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 out_features,\n",
    "                 mask,\n",
    "                 cond_in_features=None,\n",
    "                 bias=True):\n",
    "        super(MaskedLinear, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if cond_in_features is not None:\n",
    "            self.cond_linear = nn.Linear(\n",
    "                cond_in_features, out_features, bias=False)\n",
    "\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None):\n",
    "        output = F.linear(inputs, self.linear.weight * self.mask,\n",
    "                          self.linear.bias)\n",
    "        if cond_inputs is not None:\n",
    "            output += self.cond_linear(cond_inputs)\n",
    "        return output\n",
    "\n",
    "\n",
    "nn.MaskedLinear = MaskedLinear\n",
    "\n",
    "class MADE(nn.Module):\n",
    "    \"\"\" An implementation of MADE\n",
    "    (https://arxiv.org/abs/1502.03509).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_inputs,\n",
    "                 num_hidden,\n",
    "                 num_cond_inputs=None,\n",
    "                 act='relu',\n",
    "                 pre_exp_tanh=False):\n",
    "        super(MADE, self).__init__()\n",
    "\n",
    "        activations = {'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh}\n",
    "        act_func = activations[act]\n",
    "\n",
    "        input_mask = get_mask(\n",
    "            num_inputs, num_hidden, num_inputs, mask_type='input')\n",
    "        hidden_mask = get_mask(num_hidden, num_hidden, num_inputs)\n",
    "        output_mask = get_mask(\n",
    "            num_hidden, num_inputs * 2, num_inputs, mask_type='output')\n",
    "\n",
    "        self.joiner = nn.MaskedLinear(num_inputs, num_hidden, input_mask,\n",
    "                                      num_cond_inputs)\n",
    "\n",
    "        self.trunk = nn.Sequential(act_func(),\n",
    "                                   nn.MaskedLinear(num_hidden, num_hidden,\n",
    "                                                   hidden_mask), act_func(),\n",
    "                                   nn.MaskedLinear(num_hidden, num_inputs * 2,\n",
    "                                                   output_mask))\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct'):\n",
    "        if mode == 'direct':\n",
    "            h = self.joiner(inputs, cond_inputs)\n",
    "            m, a = self.trunk(h).chunk(2, 1)\n",
    "            u = (inputs - m) * torch.exp(-a)\n",
    "            return u, -a.sum(-1, keepdim=True)\n",
    "\n",
    "        else:\n",
    "            x = torch.zeros_like(inputs)\n",
    "            for i_col in range(inputs.shape[1]):\n",
    "                h = self.joiner(x, cond_inputs)\n",
    "                m, a = self.trunk(h).chunk(2, 1)\n",
    "                x[:, i_col] = inputs[:, i_col] * torch.exp(\n",
    "                    a[:, i_col]) + m[:, i_col]\n",
    "            return x, -a.sum(-1, keepdim=True)\n",
    "\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct'):\n",
    "        if mode == 'direct':\n",
    "            s = torch.sigmoid\n",
    "            return s(inputs), torch.log(s(inputs) * (1 - s(inputs))).sum(\n",
    "                -1, keepdim=True)\n",
    "        else:\n",
    "            return torch.log(inputs /\n",
    "                             (1 - inputs)), -torch.log(inputs - inputs**2).sum(\n",
    "                                 -1, keepdim=True)\n",
    "\n",
    "\n",
    "class BatchNormFlow(nn.Module):\n",
    "    \"\"\" An implementation of a batch normalization layer from\n",
    "    Density estimation using Real NVP\n",
    "    (https://arxiv.org/abs/1605.08803).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, momentum=0.0, eps=1e-5):\n",
    "        super(BatchNormFlow, self).__init__()\n",
    "\n",
    "        self.log_gamma = nn.Parameter(torch.zeros(num_inputs))\n",
    "        self.beta = nn.Parameter(torch.zeros(num_inputs))\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        self.register_buffer('running_mean', torch.zeros(num_inputs))\n",
    "        self.register_buffer('running_var', torch.ones(num_inputs))\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct'):\n",
    "        if mode == 'direct':\n",
    "            if self.training:\n",
    "                self.batch_mean = inputs.mean(0)\n",
    "                self.batch_var = (\n",
    "                    inputs - self.batch_mean).pow(2).mean(0) + self.eps\n",
    "\n",
    "                self.running_mean.mul_(self.momentum)\n",
    "                self.running_var.mul_(self.momentum)\n",
    "\n",
    "                self.running_mean.add_(self.batch_mean.data *\n",
    "                                       (1 - self.momentum))\n",
    "                self.running_var.add_(self.batch_var.data *\n",
    "                                      (1 - self.momentum))\n",
    "\n",
    "                mean = self.batch_mean\n",
    "                var = self.batch_var\n",
    "            else:\n",
    "                mean = self.running_mean\n",
    "                var = self.running_var\n",
    "\n",
    "            x_hat = (inputs - mean) / var.sqrt()\n",
    "            y = torch.exp(self.log_gamma) * x_hat + self.beta\n",
    "            return y, (self.log_gamma - 0.5 * torch.log(var)).sum(\n",
    "                -1, keepdim=True)\n",
    "        else:\n",
    "            if self.training:\n",
    "                mean = self.batch_mean\n",
    "                var = self.batch_var\n",
    "            else:\n",
    "                mean = self.running_mean\n",
    "                var = self.running_var\n",
    "\n",
    "            x_hat = (inputs - self.beta) / torch.exp(self.log_gamma)\n",
    "\n",
    "            y = x_hat * var.sqrt() + mean\n",
    "\n",
    "            return y, (-self.log_gamma + 0.5 * torch.log(var)).sum(\n",
    "                -1, keepdim=True)\n",
    "\n",
    "\n",
    "class Reverse(nn.Module):\n",
    "    \"\"\" An implementation of a reversing layer from\n",
    "    Density estimation using Real NVP\n",
    "    (https://arxiv.org/abs/1605.08803).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "        super(Reverse, self).__init__()\n",
    "        self.perm = np.array(np.arange(0, num_inputs)[::-1])\n",
    "        self.inv_perm = np.argsort(self.perm)\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct'):\n",
    "        if mode == 'direct':\n",
    "            return inputs[:, self.perm], torch.zeros(\n",
    "                inputs.size(0), 1, device=inputs.device)\n",
    "        else:\n",
    "            return inputs[:, self.inv_perm], torch.zeros(\n",
    "                inputs.size(0), 1, device=inputs.device)\n",
    "\n",
    "class FlowSequential(nn.Sequential):\n",
    "    \"\"\" A sequential container for flows.\n",
    "    In addition to a forward pass it implements a backward pass and\n",
    "    computes log jacobians.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct', logdets=None):\n",
    "        \"\"\" Performs a forward or backward pass for flow modules.\n",
    "        Args:\n",
    "            inputs: a tuple of inputs and logdets\n",
    "            mode: to run direct computation or inverse\n",
    "        \"\"\"\n",
    "        self.num_inputs = inputs.size(-1)\n",
    "\n",
    "        if logdets is None:\n",
    "            logdets = torch.zeros(inputs.size(0), 1, device=inputs.device)\n",
    "\n",
    "        assert mode in ['direct', 'inverse']\n",
    "        if mode == 'direct':\n",
    "            for module in self._modules.values():\n",
    "                inputs, logdet = module(inputs, cond_inputs, mode)\n",
    "                logdets += logdet\n",
    "        else:\n",
    "            for module in reversed(self._modules.values()):\n",
    "                inputs, logdet = module(inputs, cond_inputs, mode)\n",
    "                logdets += logdet\n",
    "\n",
    "        return inputs, logdets\n",
    "\n",
    "    def log_probs(self, inputs, cond_inputs = None):\n",
    "        u, log_jacob = self(inputs, cond_inputs)\n",
    "        log_probs = (-0.5 * u.pow(2) - 0.5 * math.log(2 * math.pi)).sum(\n",
    "            -1, keepdim=True)\n",
    "        return (log_probs + log_jacob).sum(-1, keepdim=True)\n",
    "\n",
    "    def sample(self, num_samples=None, noise=None, cond_inputs=None):\n",
    "        if noise is None:\n",
    "            noise = torch.Tensor(num_samples, self.num_inputs).normal_()\n",
    "        device = next(self.parameters()).device\n",
    "        noise = noise.to(device)\n",
    "        if cond_inputs is not None:\n",
    "            cond_inputs = cond_inputs.to(device)\n",
    "        samples = self.forward(noise, cond_inputs, mode='inverse')[0]\n",
    "        return samples\n",
    "\n",
    "print('Training of the model')\n",
    "\n",
    "def main(args):\n",
    "    # Pass config to wandb\n",
    "    for key, value in vars(args).items():\n",
    "        setattr(config, key, value)\n",
    "\n",
    "    # Use CUDA GPU if available\n",
    "    gpu_available = args.use_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if gpu_available else \"cpu\")\n",
    "\n",
    "    # Get Dataset\n",
    "    dataset = get_datasets(args.dataset_name, args.seed)\n",
    "    input_size = get_input_size(args.dataset_name)\n",
    "\n",
    "    # Make dataloader\n",
    "    train, val, test = dataset\n",
    "    train_loader = DataLoader(train, batch_size=args.batch_size)\n",
    "    val_loader = DataLoader(val, batch_size=args.batch_size)\n",
    "    test_loader = DataLoader(test, batch_size=args.batch_size)\n",
    "\n",
    "    # Define model\n",
    "    modules = []\n",
    "    for _ in range(args.made_blocks):\n",
    "        modules += [\n",
    "            MADE(input_size, args.hidden_dims, num_cond_inputs=None, act='relu'),\n",
    "            #fnn.BatchNormFlow(input_size),\n",
    "            Reverse(input_size)\n",
    "        ]\n",
    "    model = FlowSequential(*modules)\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    if args.enable_dp:\n",
    "      privacy_engine = PrivacyEngine(\n",
    "        model,\n",
    "        batch_size = args.batch_size,\n",
    "        sample_size = len(train_loader.dataset),\n",
    "        alphas = [1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
    "        noise_multiplier = args.sigma,\n",
    "        max_grad_norm=args.max_per_sample_grad_norm,\n",
    "        secure_rng=args.secure_rng,\n",
    "      )\n",
    "      privacy_engine.attach(optimizer)\n",
    "\n",
    "    # Train model\n",
    "    best_validation_loss = float('inf')\n",
    "    consecutive_bad_count = 0\n",
    "    model.train()\n",
    "    for epoch_num in range(1, args.epoch+1):\n",
    "        start = time.time()\n",
    "        # Train for 1 epoch\n",
    "        train_loss = 0\n",
    "        if args.dataset_name == 'mnist':\n",
    "          for batch, _ in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss = Negative Log Likelihood\n",
    "            loss = -model.log_probs(batch).mean()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "          for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Loss = Negative Log Likelihood\n",
    "            loss = -model.log_probs(batch).mean()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss = np.sum(train_loss) / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        if args.dataset_name == 'mnist':\n",
    "          for batch, _ in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            val_loss += -model.log_probs(batch).mean().item()\n",
    "        else:\n",
    "          for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            val_loss += -model.log_probs(batch).mean().item()\n",
    "        avg_val_loss = np.sum(val_loss) / len(val_loader)\n",
    "\n",
    "        end = time.time()\n",
    "        duration = (end-start)/60\n",
    "\n",
    "        if args.enable_dp:\n",
    "          epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(args.delta)\n",
    "        else:\n",
    "          epsilon, best_alpha = None\n",
    "        # Log statistics to wandb and stdout\n",
    "        description = f'Epoch {epoch_num:3} | duration: {duration:12.5f}| train LL: {-avg_loss:12.5f} | val LL: {-avg_val_loss:12.5f} | epsilon: {epsilon:12.5f} | best alpha: {best_alpha:12.5f}'\n",
    "        print(description)\n",
    "        wandb.log({\n",
    "            'epoch': epoch_num,\n",
    "            'average log likelihood in nats (train)': -avg_loss,\n",
    "            'average log likelihood in nats (validation)': -avg_val_loss,\n",
    "            'epsilon': epsilon,\n",
    "            'best alpha': best_alpha\n",
    "        })\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_val_loss\n",
    "            consecutive_bad_count = 0\n",
    "            torch.save(model, \"my_trained_maf.pt\") # Save best model\n",
    "        else:\n",
    "            consecutive_bad_count += 1\n",
    "        if consecutive_bad_count >= args.patience:\n",
    "            print(f'No improvement for {args.patience} epochs. Early stopping...')\n",
    "            break\n",
    "    torch.save(model, \"saved_models/\" + args.dataset_name + \"_trained_dp_model.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Training script to train differentially private normalizigng flows model\")\n",
    "    parser.add_argument('--patience', default=30, type=int, help=\"How many epochs to tolerate for early stopping\")\n",
    "    parser.add_argument('--use_cuda', default=True, type=bool, help=\"Whether to use GPU or CPU. True for GPU\")\n",
    "    parser.add_argument('--dataset_name', default='mnist', type=str, help=\"Dataset name to train on\")\n",
    "    parser.add_argument('--epoch', default=1000, type=int, help=\"number of epochs to train\")\n",
    "    parser.add_argument('--seed', default=42, type=int, help='Random seed for reproducibility')\n",
    "    parser.add_argument('--batch_size', default=128, type=int, help=\"Batch size for training model\")\n",
    "    parser.add_argument('--learning_rate', default=1e-4, type=float, help=\"Learning rate for the optimizer\")\n",
    "    parser.add_argument('--weight_decay', default=1e-6, type=float, help=\"Weight decay for the optimizer\")\n",
    "    parser.add_argument('--made_blocks', default=5, type=int, help='Number of MADE blocks for the MAF model')\n",
    "    parser.add_argument('--hidden_dims', default=512, type=int, help='Number of nodes for hidden layers for each MADE block')\n",
    "    parser.add_argument('--enable_dp', default=True, type=bool, help='Whether to train model with Differential Privacy (DP) constraints. True for DP')\n",
    "    parser.add_argument('--sigma', default=1.0, type=float, help='Noise multiplier (default 1.0)')\n",
    "    parser.add_argument('----max-per-sample-grad_norm', default=1.0, type=float, help='Clip per-sample gradients to this norm (default 1.0)')\n",
    "    parser.add_argument('--secure_rng', default=False, type=bool, help='Enable Secure RNG to have trustworthy privacy guarantees. Comes at a performance cost')\n",
    "    parser.add_argument('--delta', default=1e-5, type=float, help=\"Target delta (default: 1e-5)\")\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}